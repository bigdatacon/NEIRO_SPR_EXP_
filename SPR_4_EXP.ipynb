{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66a4833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a6a30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.10/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.10/site-packages (from triton==3.3.1->torch) (70.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b2d75",
   "metadata": {},
   "source": [
    "Задание 1\n",
    "Обновите конфигурационный файл так, чтобы в каждом из эмбеддеров обучался последний трансформерный слой/конволюционный блок. \n",
    "Запустите обучение на ВМ. При необходимости уменьшите BATCH_SIZE до 4-8. \n",
    "После выполнения задания сверьтесь с авторским решением. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разморожен слой: encoder.layer.11.attention.self.query.weight\n",
      "Разморожен слой: encoder.layer.11.attention.self.query.bias\n",
      "Разморожен слой: encoder.layer.11.attention.self.key.weight\n",
      "Разморожен слой: encoder.layer.11.attention.self.key.bias\n",
      "Разморожен слой: encoder.layer.11.attention.self.value.weight\n",
      "Разморожен слой: encoder.layer.11.attention.self.value.bias\n",
      "Разморожен слой: encoder.layer.11.attention.output.dense.weight\n",
      "Разморожен слой: encoder.layer.11.attention.output.dense.bias\n",
      "Разморожен слой: encoder.layer.11.attention.output.LayerNorm.weight\n",
      "Разморожен слой: encoder.layer.11.attention.output.LayerNorm.bias\n",
      "Разморожен слой: encoder.layer.11.intermediate.dense.weight\n",
      "Разморожен слой: encoder.layer.11.intermediate.dense.bias\n",
      "Разморожен слой: encoder.layer.11.output.dense.weight\n",
      "Разморожен слой: encoder.layer.11.output.dense.bias\n",
      "Разморожен слой: encoder.layer.11.output.LayerNorm.weight\n",
      "Разморожен слой: encoder.layer.11.output.LayerNorm.bias\n",
      "Разморожен слой: pooler.dense.weight\n",
      "Разморожен слой: pooler.dense.bias\n",
      "Разморожен слой: blocks.6.0.conv_pw.weight\n",
      "Разморожен слой: blocks.6.0.bn1.weight\n",
      "Разморожен слой: blocks.6.0.bn1.bias\n",
      "Разморожен слой: blocks.6.0.conv_dw.weight\n",
      "Разморожен слой: blocks.6.0.bn2.weight\n",
      "Разморожен слой: blocks.6.0.bn2.bias\n",
      "Разморожен слой: blocks.6.0.se.conv_reduce.weight\n",
      "Разморожен слой: blocks.6.0.se.conv_reduce.bias\n",
      "Разморожен слой: blocks.6.0.se.conv_expand.weight\n",
      "Разморожен слой: blocks.6.0.se.conv_expand.bias\n",
      "Разморожен слой: blocks.6.0.conv_pwl.weight\n",
      "Разморожен слой: blocks.6.0.bn3.weight\n",
      "Разморожен слой: blocks.6.0.bn3.bias\n",
      "Разморожен слой: conv_head.weight\n",
      "Разморожен слой: bn2.weight\n",
      "Разморожен слой: bn2.bias\n",
      "training started\n",
      "Epoch 0/29 | avg_Loss: 1.3424 | Train F1: 0.2950| Val F1: 0.5200\n",
      "New best model, epoch: 0\n",
      "Epoch 1/29 | avg_Loss: 1.2002 | Train F1: 0.4250| Val F1: 0.5450\n",
      "New best model, epoch: 1\n",
      "Epoch 2/29 | avg_Loss: 1.0986 | Train F1: 0.5100| Val F1: 0.5300\n",
      "Epoch 3/29 | avg_Loss: 1.0726 | Train F1: 0.5312| Val F1: 0.5750\n",
      "New best model, epoch: 3\n"
     ]
    }
   ],
   "source": [
    "# Сначала установите и импортируйте torch\n",
    "!pip install torch torchvision --quiet\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from utils import train\n",
    "\n",
    "class Config:\n",
    "    SEED = 42\n",
    "\n",
    "    TEXT_MODEL_NAME = \"bert-base-uncased\"\n",
    "    IMAGE_MODEL_NAME = \"tf_efficientnet_b0\"\n",
    "\n",
    "    TEXT_MODEL_UNFREEZE = \"encoder.layer.11|pooler\"\n",
    "    IMAGE_MODEL_UNFREEZE = \"blocks.6|conv_head|bn2\"\n",
    "\n",
    "    BATCH_SIZE = 256\n",
    "    TEXT_LR = 3e-5\n",
    "    IMAGE_LR = 1e-4\n",
    "    CLASSIFIER_LR = 1e-3\n",
    "    EPOCHS = 30\n",
    "    DROPOUT = 0.3\n",
    "    HIDDEN_DIM = 256\n",
    "    NUM_CLASSES = 4\n",
    "\n",
    "    TRAIN_DF_PATH = \"data/imdb_train.csv\"\n",
    "    VAL_DF_PATH = \"data/imdb_val.csv\"\n",
    "    SAVE_PATH = \"best_model.pth\"\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cfg = Config()\n",
    "\n",
    "train(cfg, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f1248",
   "metadata": {},
   "source": [
    "Задание 2\n",
    "И снова идём на ВМ. Используя функцию validate из utils.py, последовательно рассчитайте f1 метрики для модели с размороженными слоями на валидационном датасете в двух сценариях: замена текста на строку \"text\" и замена картинки на случайный тензор. Для этого адаптируйте код загрузчика данных. \n",
    "Сравните полученные значения и сделайте вывод о важности модальностей в этой задаче. Учтите, что не для каждого фильма в датасете есть постер. \n",
    "После выполнения задания сверьтесь с авторским решением. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd49a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torchmetrics\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from dataset import get_transforms, collate_fn\n",
    "from utils import MultimodalModel, validate, seed_everything\n",
    "\n",
    "\n",
    "class Config:\n",
    "    SEED = 42\n",
    "\n",
    "    TEXT_MODEL_NAME = \"bert-base-uncased\"\n",
    "    IMAGE_MODEL_NAME = \"tf_efficientnet_b0\"\n",
    "\n",
    "    TEXT_MODEL_UNFREEZE = \"encoder.layer.11|pooler\"\n",
    "    IMAGE_MODEL_UNFREEZE = \"blocks.6|conv_head|bn2\"\n",
    "\n",
    "    BATCH_SIZE = 256\n",
    "    TEXT_LR = 3e-5\n",
    "    IMAGE_LR = 1e-4\n",
    "    CLASSIFIER_LR = 1e-3\n",
    "    EPOCHS = 30\n",
    "    DROPOUT = 0.3\n",
    "    HIDDEN_DIM = 256\n",
    "    NUM_CLASSES = 4\n",
    "\n",
    "    TRAIN_DF_PATH = \"data/imdb_train.csv\"\n",
    "    VAL_DF_PATH = \"data/imdb_val.csv\"\n",
    "    SAVE_PATH = \"best_model.pth\"\n",
    "\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, config, transforms, ds_type=\"train\", mask=\"image\"):\n",
    "        if ds_type == \"train\":\n",
    "            self.df = pd.read_csv(config.TRAIN_DF_PATH)\n",
    "        else:\n",
    "            self.df = pd.read_csv(config.VAL_DF_PATH)\n",
    "        self.image_cfg = timm.get_pretrained_cfg(config.IMAGE_MODEL_NAME)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.TEXT_MODEL_NAME)\n",
    "        self.transforms = transforms\n",
    "        self.mask = mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        label = self.df.loc[idx, \"label\"]\n",
    "\n",
    "        img_path = self.df.loc[idx, \"movie_id\"]\n",
    "        try:\n",
    "            image = Image.open(f\"data/imdb_images/{img_path}.jpg\").convert('RGB')\n",
    "            if self.mask == \"image\":\n",
    "                image = torch.randint(0, 255, (*self.image_cfg.input_size[1:],\n",
    "                                           self.image_cfg.input_size[0])).to(\n",
    "                                               torch.float32)\n",
    "            else:\n",
    "                text = \"text\"\n",
    "        except:\n",
    "            image = torch.randint(0, 255, (*self.image_cfg.input_size[1:],\n",
    "                                           self.image_cfg.input_size[0])).to(\n",
    "                                               torch.float32)\n",
    "\n",
    "        image = self.transforms(image=np.array(image))[\"image\"]\n",
    "        return {\"label\": label, \"image\": image, \"text\": text}\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed_everything(config.SEED)\n",
    "\n",
    "model = MultimodalModel(config).to(device)\n",
    "state_dict = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.TEXT_MODEL_NAME)\n",
    "\n",
    "\n",
    "transforms = get_transforms(config, ds_type=\"val\")\n",
    "dataset_no_image = MultimodalDataset(config, transforms, ds_type=\"val\", mask=\"image\")\n",
    "loader = DataLoader(dataset_no_image,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=partial(collate_fn,\n",
    "                                               tokenizer=tokenizer))\n",
    "\n",
    "f1_metric_val = torchmetrics.F1Score(\n",
    "        task=\"binary\" if config.NUM_CLASSES == 2 else \"multiclass\",\n",
    "        num_classes=config.NUM_CLASSES).to(device)\n",
    "\n",
    "f1_no_image = validate(model, loader, device, f1_metric_val)\n",
    "\n",
    "f1_metric_val.reset()\n",
    "\n",
    "dataset_no_text = MultimodalDataset(config, transforms, ds_type=\"val\", mask=\"text\")\n",
    "loader = DataLoader(dataset_no_text,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=partial(collate_fn,\n",
    "                                               tokenizer=tokenizer))\n",
    "\n",
    "f1_no_text = validate(model, loader, device, f1_metric_val)\n",
    "f1_metric_val.reset()\n",
    "\n",
    "print(f\"F1-score, images masked: {f1_no_image:.3f}\")\n",
    "print(f\"F1-score, texts masked: {f1_no_text:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
